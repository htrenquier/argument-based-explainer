{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "from anchor import anchor_tabular\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = '../../../datasets/'\n",
    "\n",
    "#features_to_use = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "features_to_use = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "feature_names = ['Race', 'Alcohol', 'Junky', 'Supervised Release',\n",
    "                    'Married', 'Felony', 'WorkRelease',\n",
    "                    'Crime against Property', 'Crime against Person',\n",
    "                    'Gender', 'Priors', 'YearsSchool', 'PrisonViolations',\n",
    "                    'Age', 'MonthsServed', '', 'Recidivism']\n",
    "def violations_fn(x):\n",
    "    x = x.astype(float)\n",
    "    d = np.digitize(x, [0, 5, float('inf')],\n",
    "                    right=True).astype('|S128')\n",
    "    return utils.map_array_values(d, {'0': 'NO', '1': '1 to 5', '2': 'More than 5'})\n",
    "def priors_fn(x):\n",
    "    x = x.astype(float)\n",
    "    d = np.digitize(x, [-1, 0, 5, float('inf')],\n",
    "                    right=True).astype('|S128')\n",
    "    return utils.map_array_values(d, {'0': 'UNKNOWN', '1': 'NO', '2': '1 to 5', '3': 'More than 5'})\n",
    "transformations = {\n",
    "    0: lambda x: utils.replace_binary_values(x, ['Black', 'White']),\n",
    "    1: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    2: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    3: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    4: lambda x: utils.replace_binary_values(x, ['No', 'Married']),\n",
    "    5: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    6: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    7: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    8: lambda x: utils.replace_binary_values(x, ['No', 'Yes']),\n",
    "    9: lambda x: utils.replace_binary_values(x, ['Female', 'Male']),\n",
    "    10: lambda x: priors_fn(x),\n",
    "    12: lambda x: violations_fn(x),\n",
    "    13: lambda x: (x.astype(float) / 12).astype(int),\n",
    "    16: lambda x: utils.replace_binary_values(x, ['No more crimes',\n",
    "                                            'Re-arrested'])\n",
    "}\n",
    "\n",
    "dataset = utils.load_csv_dataset(\n",
    "    os.path.join(dataset_folder, 'recidivism/Data_1980.csv'), 16,\n",
    "    feature_names=feature_names, discretize=True,\n",
    "    features_to_use=features_to_use, balance=True,\n",
    "    feature_transformations=transformations, skip_first=True)\n",
    "#dataset = utils.load_dataset('recidivism', balance=True, dataset_folder=dataset_folder, discretize=True)\n",
    "print(dataset.train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.7456965394853594\n",
      "Test 0.6411347517730497\n"
     ]
    }
   ],
   "source": [
    "c = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "c.fit(dataset.train, dataset.labels_train)\n",
    "#print(np.unique(dataset.labels_train))\n",
    "print('Train', sklearn.metrics.accuracy_score(dataset.labels_train, c.predict(dataset.train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(dataset.labels_test, c.predict(dataset.test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '0', '0', '0', '0', '1', '0', '1', '1', '8.00 < YearsSchool <= 10.00']\n",
      "(5635, 11)\n",
      "[0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      "[[0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "transformed_data = []\n",
    "\n",
    "for r in dataset.train:\n",
    "    transformed_data.append([dataset.categorical_names[i][int(r_)] for i, r_ in enumerate(r)])        \n",
    "\n",
    "print(transformed_data[0])\n",
    "\n",
    "train_data = pd.DataFrame(transformed_data, columns=dataset.feature_names)\n",
    "print(train_data.shape)\n",
    "nb_vals = 50\n",
    "\n",
    "print(dataset.train[0])\n",
    "print(dataset.train[0].reshape(1, -1))\n",
    "y=[]\n",
    "for k in range(0, nb_vals):\n",
    "    y.append(c.predict(dataset.train[k].reshape(1, -1))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Race', 'Alcohol', 'Junky', 'Married', 'Felony', 'WorkRelease', 'Crime against Property', 'Crime against Person', 'Gender', 'Priors', 'YearsSchool']\n",
      "Generating arguments\n",
      "len  1 : 0 ,  0\n",
      "25 potential arg checked ( 0 not minimal)\n",
      "len  2 : 24 ,  24\n",
      "263 potential arg checked ( 0 not minimal)\n",
      "len  3 : 62 ,  50\n",
      "1007 potential arg checked ( 437 not minimal)\n",
      "len  4 : 28 ,  45\n",
      "2165 potential arg checked ( 2471 not minimal)\n",
      "len  5 : 3 ,  11\n",
      "2969 potential arg checked ( 6382 not minimal)\n",
      "len  6 : 0 ,  1\n",
      "2613 potential arg checked ( 9720 not minimal)\n",
      "len  7 : 0 ,  0\n",
      "1454 potential arg checked ( 9363 not minimal)\n",
      "len  8 : 0 ,  0\n",
      "497 potential arg checked ( 5781 not minimal)\n",
      "len  9 : 0 ,  0\n",
      "96 potential arg checked ( 2232 not minimal)\n",
      "len  10 : 0 ,  0\n",
      "8 potential arg checked ( 494 not minimal)\n",
      "len  11 : 0 ,  0\n",
      "0 potential arg checked ( 48 not minimal)\n",
      "Saving to  ../../saves\\recidivismshort_50_minimals.df\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(sys.path[0] + '/../..')\n",
    "\n",
    "import arg_explainer as ae\n",
    "from importlib import reload\n",
    "import MinimaliT\n",
    "reload(ae)\n",
    "reload(MinimaliT)\n",
    "import arg_explainer as ae\n",
    "\n",
    "print(dataset.feature_names)\n",
    "explainer = ae.ArgTabularExplainer(c, train_data.iloc[0:nb_vals], y, 'recidivismshort_' + str(nb_vals), compute=True, output_path='../../saves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recidivismshort_50\n",
      "edges per node: 54.79838709677419\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "G = explainer.build_attack_graph(compute=True, display_graph=False)\n",
    "print('edges per node:', np.mean([len(G.edges(n)) for n in G.nodes()]))\n",
    "\n",
    "explainer.export_graph('asp', '../../saves')\n",
    "print(len(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "total length 23040 23040\n",
      "Generating arguments\n",
      "len  1 : 0 ,  0\n",
      "29 potential arg checked ( 0 not minimal)\n",
      "len  2 : 0 ,  0\n",
      "382 potential arg checked ( 0 not minimal)\n",
      "len  3 : 0 ,  0\n",
      "3024 potential arg checked ( 0 not minimal)\n",
      "len  4 : 0 ,  0\n",
      "16032 potential arg checked ( 0 not minimal)\n",
      "len  5 : 5 ,  1\n",
      "60000 potential arg checked ( 0 not minimal)\n",
      "len  6 : 348 ,  126\n",
      "162539 potential arg checked ( 85 not minimal)\n",
      "len  7 : 1202 ,  856\n",
      "316908 potential arg checked ( 4884 not minimal)\n",
      "len  8 : 2280 ,  2231\n",
      "430661 potential arg checked ( 30907 not minimal)\n",
      "len  9 : 2834 ,  2785\n",
      "379633 potential arg checked ( 88591 not minimal)\n",
      "len  10 : 1245 ,  1332\n",
      "187621 potential arg checked ( 131353 not minimal)\n",
      "len  11 : 208 ,  260\n",
      "37381 potential arg checked ( 93270 not minimal)\n",
      "len  12 : 0 ,  2\n",
      "2 potential arg checked ( 23038 not minimal)\n",
      "Saving to  ../../saves\\rcdvshort_23040_synth_minimals.df\n",
      "rcdvshort_23040_synth\n",
      "total args: 15715\n",
      "edges per node: 2.5225580655424755\n",
      "total length 24576 24576\n",
      "Generating arguments\n",
      "len  1 : 0 ,  0\n",
      "29 potential arg checked ( 0 not minimal)\n",
      "len  2 : 0 ,  0\n",
      "382 potential arg checked ( 0 not minimal)\n",
      "len  3 : 0 ,  0\n",
      "3024 potential arg checked ( 0 not minimal)\n",
      "len  4 : 0 ,  0\n",
      "16032 potential arg checked ( 0 not minimal)\n",
      "len  5 : 5 ,  1\n",
      "60000 potential arg checked ( 0 not minimal)\n",
      "len  6 : 322 ,  124\n",
      "162539 potential arg checked ( 85 not minimal)\n",
      "len  7 : 1168 ,  819\n",
      "317130 potential arg checked ( 4662 not minimal)\n",
      "len  8 : 2119 ,  1980\n",
      "431725 potential arg checked ( 29843 not minimal)\n",
      "len  9 : 2645 ,  2494\n",
      "383246 potential arg checked ( 84978 not minimal)\n",
      "len  10 : 1015 ,  1155\n",
      "193182 potential arg checked ( 125794 not minimal)\n",
      "len  11 : 81 ,  128\n",
      "41273 potential arg checked ( 89799 not minimal)\n",
      "len  12 : 2 ,  3\n",
      "5 potential arg checked ( 24571 not minimal)\n",
      "Saving to  ../../saves\\rcdvshort_24576_synth_minimals.df\n",
      "rcdvshort_24576_synth\n",
      "total args: 14061\n",
      "edges per node: 0.0\n"
     ]
    }
   ],
   "source": [
    "from typing import OrderedDict\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from numba import njit\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "def generate_full_dataset():\n",
    "    instance = OrderedDict.fromkeys(explainer.dataset.columns)\n",
    "    full_dataset = []\n",
    "\n",
    "    def gen_rec(instance, columns, col_index, full_dataset):\n",
    "        inst_ = instance.copy()\n",
    "        if col_index == len(columns) - 1:\n",
    "            for f in explainer.features_p_col[columns[col_index]]:\n",
    "                inst_[columns[col_index]] = explainer.feature_names[f][len(columns[col_index])+1:] #.strip(columns[col_index] + '_')\n",
    "                full_dataset.append(list(inst_.values()))\n",
    "        else:\n",
    "            inst_ = instance.copy()\n",
    "            for f in explainer.features_p_col[columns[col_index]]:\n",
    "                inst_[columns[col_index]] = explainer.feature_names[f][len(columns[col_index])+1:] #.strip(columns[col_index] + '_')\n",
    "                gen_rec(inst_, columns, col_index + 1, full_dataset)\n",
    "\n",
    "    gen_rec(instance, list(instance.keys()), 0, full_dataset)\n",
    "    return full_dataset\n",
    "\n",
    "def instance2encoded(instance, dataset):\n",
    "    encoded = []\n",
    "    for col in dataset.categorical_features:\n",
    "        encoded.append(dataset.categorical_names[col].index(instance[col]))\n",
    "    return np.array(encoded)\n",
    "\n",
    "def explore_full_dataset(nb_steps):\n",
    "    full_dataset = generate_full_dataset()\n",
    "    y_plus = [c.predict(instance2encoded(i_, dataset).reshape(1,-1))[0] for i_ in full_dataset]\n",
    "    #train_data_plus = train_data = pd.DataFrame(transformed_data[:nb_vals] + transformed_data_plus, columns=dataset.feature_names)\n",
    "    \n",
    "    random.seed(1)\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    full_dataset_shuff = [full_dataset[i] for i in indices] \n",
    "    y_plus_shuff = [y_plus[i] for i in indices]\n",
    "\n",
    "    print(np.unique(np.array(y_plus_shuff)))\n",
    "    \n",
    "    step_len = len(full_dataset)//nb_steps\n",
    "    steps = [i*step_len for i in range(15, nb_steps)]\n",
    "    steps.append(len(full_dataset))\n",
    "    for nb_vals in steps:\n",
    "        dataset_t = full_dataset_shuff[:nb_vals]\n",
    "        y_t = y_plus_shuff[:nb_vals]\n",
    "        print('total length', len(dataset_t), len(y_t))\n",
    "        train_data_plus = pd.DataFrame(dataset_t, columns=dataset.feature_names)\n",
    "\n",
    "        explainer = ae.ArgTabularExplainer(c, train_data_plus, y_t, 'rcdvshort_' + str(nb_vals) + '_synth', compute=True, output_path='../../saves')\n",
    "\n",
    "        G = explainer.build_attack_graph(compute=True, display_graph=False)\n",
    "        print('total args:', len(G.nodes()))\n",
    "        print('edges per node:', np.mean([len(G.edges(n)) for n in G.nodes()]))\n",
    "\n",
    "explore_full_dataset(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recidivism_1000_atk_graph.df', 'recidivism_100_atk_graph.df', 'recidivism_200_atk_graph.df', 'recidivism_500_atk_graph.df', 'recidivism_50_atk_graph.df'] ['recidivism_1000_R_atk.df', 'recidivism_100_R_atk.df', 'recidivism_200_R_atk.df', 'recidivism_500_R_atk.df', 'recidivism_50_R_atk.df']\n",
      "[117395305, 1745735, 8820614, 50879238, 302275]\n"
     ]
    }
   ],
   "source": [
    "explainer.af_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-processed file...\n",
      "Reading ../../saves/recidivismshort_50_sat_preprocessed.txt\n",
      "Time for selection:  1404.9758040904999\n",
      "Len max_cov_exts:  1\n",
      "id: 0 coverage: 8 Arg 1/4: frozenset({'Felony_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 1 coverage: 16 Arg 1/3: frozenset({'Married_0', 'Race_0', 'Priors_2'})\n",
      "id: 2 coverage: 10 Arg 1/1: frozenset({'Race_1', 'Married_1'})\n",
      "id: 3 coverage: 3 Arg 1/1: frozenset({'Race_0', 'YearsSchool_10.00 < YearsSchool <= 11.00'})\n",
      "id: 4 coverage: 1 Arg 1/1: frozenset({'Felony_0', 'Crime against Property_1', 'Race_1', 'Priors_1'})\n",
      "id: 5 coverage: 0 Arg 1/0: None\n",
      "id: 6 coverage: 4 Arg 1/1: frozenset({'Crime against Property_0', 'YearsSchool_YearsSchool > 11.00', 'Priors_1'})\n",
      "id: 7 coverage: 7 Arg 1/3: frozenset({'Felony_0', 'Race_0', 'Priors_2'})\n",
      "id: 8 coverage: 10 Arg 1/1: frozenset({'Race_1', 'Married_1'})\n",
      "id: 9 coverage: 1 Arg 1/1: frozenset({'Priors_2', 'Alcohol_0', 'Race_1', 'WorkRelease_0'})\n",
      "id: 10 coverage: 5 Arg 1/1: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Crime against Property_0', 'Priors_1'})\n",
      "id: 11 coverage: 4 Arg 1/1: frozenset({'Alcohol_1', 'Race_1', 'Priors_1'})\n",
      "id: 12 coverage: 10 Arg 1/1: frozenset({'Race_1', 'Married_1'})\n",
      "id: 13 coverage: 12 Arg 1/7: frozenset({'Felony_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 14 coverage: 4 Arg 1/1: frozenset({'Alcohol_1', 'Race_1', 'Priors_1'})\n",
      "id: 15 coverage: 1 Arg 1/1: frozenset({'Crime against Property_0', 'Priors_1', 'Married_0', 'Alcohol_0', 'YearsSchool_YearsSchool <= 8.00'})\n",
      "id: 16 coverage: 6 Arg 1/2: frozenset({'Alcohol_1', 'Married_0', 'Priors_2'})\n",
      "id: 17 coverage: 10 Arg 1/1: frozenset({'Race_1', 'Married_1'})\n",
      "id: 18 coverage: 13 Arg 1/2: frozenset({'Race_1', 'Married_1'})\n",
      "id: 19 coverage: 6 Arg 1/2: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Crime against Property_0', 'Priors_1'})\n",
      "id: 20 coverage: 4 Arg 1/1: frozenset({'Priors_0', 'Race_0'})\n",
      "id: 21 coverage: 5 Arg 1/1: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Crime against Property_0', 'Priors_1'})\n",
      "id: 22 coverage: 4 Arg 1/1: frozenset({'Priors_0', 'Race_0'})\n",
      "id: 23 coverage: 13 Arg 1/2: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Race_1', 'Priors_1'})\n",
      "id: 24 coverage: 0 Arg 1/0: None\n",
      "id: 25 coverage: 11 Arg 1/4: frozenset({'Felony_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 26 coverage: 4 Arg 1/1: frozenset({'Crime against Property_0', 'YearsSchool_YearsSchool > 11.00', 'Priors_1'})\n",
      "id: 27 coverage: 6 Arg 1/2: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Crime against Property_0', 'Priors_1'})\n",
      "id: 28 coverage: 11 Arg 1/2: frozenset({'Alcohol_0', 'Crime against Property_0', 'Race_1', 'Priors_2'})\n",
      "id: 29 coverage: 7 Arg 1/4: frozenset({'YearsSchool_YearsSchool > 11.00', 'Race_0', 'Priors_2'})\n",
      "id: 30 coverage: 8 Arg 1/3: frozenset({'Married_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 31 coverage: 11 Arg 1/2: frozenset({'Alcohol_0', 'Crime against Property_0', 'Race_1', 'Priors_2'})\n",
      "id: 32 coverage: 3 Arg 1/1: frozenset({'Alcohol_0', 'Crime against Property_0', 'Race_1', 'Priors_2'})\n",
      "id: 33 coverage: 5 Arg 1/1: frozenset({'Married_0', 'WorkRelease_1', 'Priors_2', 'Crime against Property_1'})\n",
      "id: 34 coverage: 7 Arg 1/4: frozenset({'Felony_0', 'Race_0', 'Priors_2'})\n",
      "id: 35 coverage: 5 Arg 1/1: frozenset({'Married_0', 'WorkRelease_1', 'Priors_2', 'Crime against Property_1'})\n",
      "id: 36 coverage: 4 Arg 1/1: frozenset({'Priors_0', 'Race_0'})\n",
      "id: 37 coverage: 6 Arg 1/2: frozenset({'YearsSchool_8.00 < YearsSchool <= 10.00', 'Crime against Property_0', 'Priors_1'})\n",
      "id: 38 coverage: 11 Arg 1/2: frozenset({'Race_1', 'YearsSchool_10.00 < YearsSchool <= 11.00', 'Priors_1'})\n",
      "id: 39 coverage: 2 Arg 1/1: frozenset({'Alcohol_1', 'Married_0', 'Priors_2'})\n",
      "id: 40 coverage: 5 Arg 1/2: frozenset({'Race_1', 'YearsSchool_10.00 < YearsSchool <= 11.00', 'Priors_1'})\n",
      "id: 41 coverage: 4 Arg 1/1: frozenset({'Crime against Property_0', 'YearsSchool_YearsSchool > 11.00', 'Priors_1'})\n",
      "id: 42 coverage: 13 Arg 1/2: frozenset({'Crime against Property_0', 'YearsSchool_YearsSchool > 11.00', 'Priors_1'})\n",
      "id: 43 coverage: 5 Arg 1/1: frozenset({'Married_0', 'WorkRelease_1', 'Priors_2', 'Crime against Property_1'})\n",
      "id: 44 coverage: 10 Arg 1/4: frozenset({'Crime against Property_1', 'Race_0', 'Priors_1'})\n",
      "id: 45 coverage: 8 Arg 1/3: frozenset({'Felony_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 46 coverage: 12 Arg 1/6: frozenset({'Felony_0', 'Race_0', 'Crime against Property_1'})\n",
      "id: 47 coverage: 7 Arg 1/4: frozenset({'Felony_0', 'Race_0', 'Priors_2'})\n",
      "id: 48 coverage: 3 Arg 1/1: frozenset({'Race_0', 'YearsSchool_10.00 < YearsSchool <= 11.00'})\n",
      "id: 49 coverage: 1 Arg 1/1: frozenset({'Priors_0', 'Race_1'})\n",
      "success =  0.96 (empty= 2 )\n"
     ]
    }
   ],
   "source": [
    "#ext_gen = explainer.extension_generator_from_graph()\n",
    "ext_gen = explainer.extension_generator_from_sat(file='../../saves/recidivismshort_50_sat.txt')\n",
    "#max_covi_ext = explainer.make_selection('max_covi_incl', ext_gen)\n",
    "max_covi_ext = explainer.make_selection('max_covi', ext_gen)\n",
    "res = explainer.apply_inference(max_covi_ext, 'universal')\n",
    "#print(res)\n",
    "\n",
    "explainer.display_explanations(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "34f4a23e072925201d1edce259708610e2f98260d5449b97ac01b140140f4b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"demos_argumentation.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMfKZfWb1T7oP1tAnJFqxSN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Args generation on tabular data\n","Examples from Leila's paper"],"metadata":{"id":"Xpzy2CGyrtzg"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","from itertools import combinations\n","from tqdm import tqdm\n","from pprint import pprint"],"metadata":{"id":"U_yuOz94pne0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","ex_labels = [0, 0, 1, 0]"],"metadata":{"id":"rrhqvs5IpoVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hiking_ex = [[0, 0, 1, 0],\n","             [1, 0, 0, 0],\n","             [0, 0, 1, 1],\n","             [1, 0, 0, 1],\n","             [0, 1, 1, 0],\n","             [0, 1, 1, 1],\n","             [1, 1, 0, 1]]\n","#            [1, 1, 0, 0]]\n","hiking_labels = [0, 1, 0, 1, 0, 0, 1]  # ,1]"],"metadata":{"id":"CdByTVt8psTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["oh_enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n","X = oh_enc.fit_transform(hiking_ex).todok()\n","\n","features_name_hiking = oh_enc.get_feature_names_out(['V', 'C', 'M', 'E'])\n","t_X = X.transpose().toarray()\n","\n","print(X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rZY7ob2pu1w","executionInfo":{"status":"ok","timestamp":1645212617614,"user_tz":-60,"elapsed":47,"user":{"displayName":"Henri Trenquier","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18067802191641776965"}},"outputId":"dfd9a433-cf1a-44f5-9ef6-1cf611326df9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 0. 1. 0. 0. 1. 1. 0.]\n"," [0. 1. 1. 0. 1. 0. 1. 0.]\n"," [1. 0. 1. 0. 0. 1. 0. 1.]\n"," [0. 1. 1. 0. 1. 0. 0. 1.]\n"," [1. 0. 0. 1. 0. 1. 1. 0.]\n"," [1. 0. 0. 1. 0. 1. 0. 1.]\n"," [0. 1. 0. 1. 1. 0. 0. 1.]]\n"]}]},{"cell_type":"code","source":["instances_by_feature = {}\n","\n","for i, col in enumerate(t_X):\n","    instances_by_feature.update({i: list(np.where(col)[0])})\n","\n","print(instances_by_feature)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgfFEhoPp-Jo","executionInfo":{"status":"ok","timestamp":1645212617615,"user_tz":-60,"elapsed":43,"user":{"displayName":"Henri Trenquier","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18067802191641776965"}},"outputId":"542c63fd-07a6-478d-efec-a16b57c93c10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: [0, 2, 4, 5], 1: [1, 3, 6], 2: [0, 1, 2, 3], 3: [4, 5, 6], 4: [1, 3, 6], 5: [0, 2, 4, 5], 6: [0, 1, 4], 7: [2, 3, 5, 6]}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PduW8K1paDp"},"outputs":[],"source":["def generate_args_lenN(n, ibyf, dataset, predictions, minimals=None):\n","    \"\"\"\n","    Generates arguments of length n, given arguments of length 1.. n-1\n","    :param n: length of arguments to be generated\n","    :param ibyf: instances_by_feature\n","    :param predictions:\n","    :param minimals: arguments (minimal)\n","    :return:\n","    \"\"\"\n","\n","    def is_minimal(potential_arg, cl, minimals, n):\n","        # cl is class\n","        set_potential_arg = set(potential_arg)\n","        for k in range(n):\n","            for comb_ in combinations(potential_arg, k+1):\n","                if frozenset(comb_) in minimals[cl][k]:\n","                    return False\n","        return True\n","\n","    if minimals is None:\n","        minimals = ([], [])\n","    assert len(minimals[0]) == n-1\n","    minimals[0].append(set())\n","    minimals[1].append(set())\n","\n","    args = [set(), set()]\n","    potential_args_checked_count = 0\n","    for i, row in enumerate(dataset):\n","        for potential_arg in combinations(np.where(row)[0], n):\n","            cl = predictions[i]\n","            potential_args_checked_count += 1\n","            if not is_minimal(potential_arg, cl, minimals, n-1):\n","                continue\n","            selection = set.intersection(*[set(ibyf[w]) for w in potential_arg])  # all rows with all features of potential argument\n","            selection_preds = [predictions[i_] for i_ in selection]\n","            if selection_preds[:-1] == selection_preds[1:]:\n","                    args[selection_preds[0]].add(frozenset(potential_arg))\n","                    minimals[cl][n-1].add(frozenset(potential_arg))\n","    print(potential_args_checked_count, ' potential arg checked.')\n","    return args, minimals\n","\n","  \n","\n","def read_args(minimals, feature_names):\n","    arguments = [[], []]\n","    for cl in range(len(minimals)):\n","        for a in range(len(minimals[cl])):\n","            for f in minimals[cl][a]:\n","                arguments[cl].append(tuple([feature_names[k] for k in f]))\n","    return arguments\n"]},{"cell_type":"code","source":["n = 0\n","minimals = None\n","print(\"len \", n, \":\", minimals)\n","while not minimals or len(minimals[0][-1]) != 0 or len(minimals[1][-1]) != 0 :\n","    n += 1\n","    args, minimals = generate_args_lenN(n, instances_by_feature, X.toarray(), hiking_labels, minimals)\n","    print(\"len \", n, \":\", minimals)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esxGyyV1plw_","executionInfo":{"status":"ok","timestamp":1645212617616,"user_tz":-60,"elapsed":35,"user":{"displayName":"Henri Trenquier","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18067802191641776965"}},"outputId":"1cbb9165-342c-42cc-b8c6-755d2cc69e58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len  0 : None\n","28  potential arg checked.\n","len  1 : ([{frozenset({5}), frozenset({0})}], [{frozenset({1}), frozenset({4})}])\n","42  potential arg checked.\n","len  2 : ([{frozenset({5}), frozenset({0})}, {frozenset({3, 6})}], [{frozenset({1}), frozenset({4})}, set()])\n","28  potential arg checked.\n","len  3 : ([{frozenset({5}), frozenset({0})}, {frozenset({3, 6})}, set()], [{frozenset({1}), frozenset({4})}, set(), set()])\n"]}]},{"cell_type":"code","source":["pprint(read_args(minimals, features_name_hiking))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOSrRG7Xq0sM","executionInfo":{"status":"ok","timestamp":1645212617617,"user_tz":-60,"elapsed":28,"user":{"displayName":"Henri Trenquier","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18067802191641776965"}},"outputId":"56307fad-9457-44a2-d554-2b1e03b552ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[('M_1',), ('V_0',), ('C_1', 'E_0')], [('V_1',), ('M_0',)]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"tvnoye_uMR8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Args generation on Text data\n","### Data preparation"],"metadata":{"id":"uGAqNIAhsFRE"}},{"cell_type":"code","source":["from IPython.display import clear_output"],"metadata":{"id":"SoyzAJsetNWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm\n","!pip install anchor-exp fasttext\n","clear_output()"],"metadata":{"id":"JH5wCIO_s5Pa","outputId":"ec06966c-d6b1-4fa2-efa1-17c53160555e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 587 kB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Collecting anchor-exp\n","  Downloading anchor_exp-0.0.2.0.tar.gz (427 kB)\n","\u001b[K     |████████████████████████████████| 427 kB 4.4 MB/s \n"]}]},{"cell_type":"code","source":["import anchor\n","import spacy\n","import fasttext\n","import os\n","import sklearn.model_selection\n","from anchor import anchor_text\n","from tqdm import tqdm\n","import numpy as np\n","from pprint import pprint\n","import itertools\n","import pandas as pd\n","from functools import reduce\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","from operator import itemgetter"],"metadata":{"id":"vwlASUfjsK_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ls \"/content/gdrive/MyDrive/Colab Notebooks/datasets/rt-polaritydata/rt-polaritydata\"\n","%cd \"/content/gdrive/MyDrive/Colab Notebooks/wd/argumentation\"\n","!pwd"],"metadata":{"id":"5MTDFk_cvqEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_polarity(path):\n","    data = []\n","    labels = []\n","    f_names = ['rt-polarity.neg', 'rt-polarity.pos']\n","    for (l, f) in enumerate(f_names):\n","        for line in open(os.path.join(path, f), 'rb'):\n","            try:\n","                line.decode('utf8')\n","            except:\n","                continue\n","            line = str(line.strip()).lstrip(\"b\\'\").rstrip(\"\\'\")\n","            line = line.strip(\"\\\"\")\n","            line = line.replace('\\\"', '\\'')\n","            data.append(line)\n","            labels.append(l)\n","    return data, labels\n","\n","def write_file(dataset, labels, file_name):\n","    flag = \"__label__\"\n","    with open(file_name, 'w') as f:\n","        for i in tqdm(range(len(dataset))):\n","            txt = str(dataset).lstrip(\"b\\'\").rstrip(\"\\'\")\n","            line = flag + str(labels[i]) + flag + \" \" + str(dataset[i])\n","            f.write(line + '\\n')"],"metadata":{"id":"qry-peLEwIyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data, labels = load_polarity(\"/content/gdrive/MyDrive/Colab Notebooks/datasets/rt-polaritydata/rt-polaritydata\")\n","train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=.2, random_state=42)\n","train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1, random_state=42)\n","\n","print('data sample:')\n","print(train[0])\n","\n","re_write_files = False\n","\n","if re_write_files:\n","    print(\"Writing train\")\n","    write_file(train, train_labels, 'rt2.train')\n","    print(\"Writing dev\")\n","    write_file(val, val_labels, 'rt2.dev')\n","    print(\"Writing test\")\n","    write_file(test, test_labels, 'rt2.test')\n","\n","re_train = False\n","if re_train:\n","    rt_model = fasttext.train_supervised(input=\"rt2.train\")\n","    rt_model.save_model(\"rt2.model\")\n","else:\n","    rt_model = fasttext.load_model(\"rt2.model\")"],"metadata":{"id":"XtOeJ3Zzyigp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Args generation on Text data\n","### Tests"],"metadata":{"id":"SeutqLTJ0r-M"}},{"cell_type":"code","source":["def predict_rt(sample):\n","    res = rt_model.predict(sample)\n","    preds = []\n","    for e in np.array(res[0]):\n","        if e[0] == '__label__1__label__': #POSITIVE\n","            preds.append(1)\n","        else:\n","            preds.append(0)\n","    return np.array(preds)\n","\n","def exemple_explain_with_anchor():\n","    nlp = spacy.load(\"en_core_web_sm\")\n","\n","    explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=False)\n","\n","    text = \"It is a good movie\"\n","    #text='the latest installment in the pokemon canon' # , pokemon 4ever is surprising less moldy and trite than the last two , likely because much of the japanese anime is set in a scenic forest where pokemon graze in peace .'\n","    pred = explainer.class_names[predict_rt([text])[0]]\n","    alternative = explainer.class_names[1 - predict_rt([text])[0]]\n","    print('Prediction: %s' % pred)\n","    exp = explainer.explain_instance(text, predict_rt, threshold=0.95)\n","\n","    print('Anchor: %s' % (' AND '.join(exp.names())))\n","    print('Precision: %.2f' % exp.precision())\n","    print()\n","    print('Examples where anchor applies and model predicts %s:' % pred)\n","    print()\n","    print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n","    print()\n","    print('Examples where anchor applies and model predicts %s:' % alternative)\n","    print()\n","    print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))\n","    print(rt_model.predict(\"Definitely not a good movie\"))\n","\n","\n","def time_test():\n","    import time\n","    text = 'the latest installment in the pokemon canon, pokemon 4ever is surprising less moldy and trite than the last two , likely because much of the japanese anime is set in a scenic forest where pokemon graze in peace .'\n","    tab_text = text.split()\n","    for i in range(1, len(tab_text)):\n","        start = time.time()\n","        text_ = \" \".join(tab_text[:i])\n","        print(text_)\n","        nlp = spacy.load(\"en_core_web_sm\")\n","        explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=False)\n","        pred = explainer.class_names[predict_rt([text_])[0]]\n","        alternative = explainer.class_names[1 - predict_rt([text_])[0]]\n","        print('Prediction: %s' % pred)\n","        exp = explainer.explain_instance(text_, predict_rt, threshold=0.95)\n","        print('Anchor: %s' % (' AND '.join(exp.names())))\n","        print('Precision: %.2f' % exp.precision())\n","        print(\"time for \" + str(i) + \"words :\", time.time() - start)\n","\n","\n","def evaluate_coherence(file=None):\n","    texts = []\n","    anchors = []\n","    predictions = []\n","    if file is not None:\n","        with open(file, 'r') as f:\n","            for line in f.readlines():\n","                s = line.split(\"\\\"\")\n","                if len(s) > 3:\n","                    continue\n","                assert s[0] == 'b'\n","                text = s[1]\n","                info = s[2].split(\",\")\n","                pred = info[2]\n","                anchor = info[3].split(\"AND\")\n","                texts.append(text)\n","                anchors.append(anchor)\n","                predictions.append(pred)\n","    assert len(texts) == len(anchors) and len(anchors) == len(predictions)\n","    print(\"anchors len:\", len(anchors))\n","    incoherences = {}\n","    for i in range(len(anchors)):\n","        incoherent = []\n","        if tuple(anchors[i]) in incoherences.keys():\n","            incoherences[tuple(anchors[i])][0].append(i)\n","            continue\n","        for j in range(len(texts)):\n","            if i == j or predictions[i] == predictions[j]:\n","                continue\n","            count = 0\n","            for word in anchors[i]:\n","                if word in texts[j]:\n","                    count += 1\n","            if count == len(anchors[i]):\n","                incoherent.append(j)\n","        if incoherent:\n","            if len(incoherent) >= 10:\n","                incoherences.update({tuple(anchors[i]): ([i], [0], len(incoherent)/len(anchors))})\n","            else:\n","                incoherences.update({tuple(anchors[i]): ([i], incoherent, len(incoherent)/len(anchors))})\n","    print(\"Anchor : ([of instances], [instances that contain anchor but have different predicition or 0 if too many], coverage\")\n","    pprint(incoherences)\n","\n","\n","# \"input, ground_truth, prediction, explanation, precision \"\n","def write_explanations(dataset, filename=\"rt2_test.explanations\"):\n","    with open(filename, \"w\") as f:\n","        for text_, gt in tqdm(dataset):\n","            pred_ = explainer.class_names[predict_rt([text_])[0]]\n","            exp_ = explainer.explain_instance(text_, predict_rt, threshold=0.5)\n","            anchor = ' AND '.join(exp_.names())\n","            line = ','.join([(\"b\\\"\" + text_ + \"\\\"\"), gt, pred_, anchor, str(exp_.precision())])\n","            f.write(line + '\\n')\n","            #print(line)"],"metadata":{"id":"MrgE9Fcn0rjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Args generation on Text data\n","### Arguments generation"],"metadata":{"id":"2YndiPno1Onz"}},{"cell_type":"code","source":["def get_dataset_from_file(file):\n","    dataset = []\n","    flag = \"__label__\"\n","    with open(file, \"r\") as f:\n","        for l in f.readlines():\n","            s = l.split(flag)\n","            gt = s[1].strip(flag)\n","            try:\n","                text = s[2].strip(\"\\n\").strip(\"\\\"\").strip(\"\\'\")\n","            except IndexError:\n","                print(s)\n","                print(l)\n","                raise IndexError\n","            dataset.append((text, gt))\n","    return dataset\n","\n","\n","def get_predictions(dataset, model=rt_model):\n","    \"\"\"\n","    Returns a list of predictions of texts in dataset. Format is int 0 (negative) or 1 (positive)\n","    :param dataset:\n","    :param model:\n","    :return:\n","    \"\"\"\n","    all_texts = [text for text, _ in dataset]\n","    res = rt_model.predict(all_texts)\n","    assert len(all_texts) == len(res[0])\n","    flag = '__label__'\n","    return [int(label.strip(flag)) for label in np.array(res[0]).squeeze()]"],"metadata":{"id":"0kYW3lJR1I2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# combination not in alphabetical order, potential_args_checked not kept for later\n","def generate_args_lenN_by_text(n, texts_by_word, predictions, minimals=None):\n","\n","    def is_minimal(potential_arg, cl, minimals, n):\n","        # cl is class\n","        set_potential_arg = set(potential_arg)\n","        for k in range(n):\n","            for comb_ in itertools.combinations(sorted(potential_arg), k+1):\n","                if frozenset(comb_) in minimals[cl][k]:\n","                    return False\n","        return True\n","\n","    if minimals is None:\n","        minimals = ([], [])\n","    assert len(minimals[0]) == n-1\n","    minimals[0].append(set())\n","    minimals[1].append(set())\n","\n","    args = [set(), set()]\n","    potential_args_checked_count = 0\n","    for i, text in tqdm(enumerate(all_split_texts)):\n","        for potential_arg in itertools.combinations(sorted(text), n):\n","            cl = predictions[i]\n","            potential_args_checked_count += 1\n","            if not is_minimal(potential_arg, cl, minimals, n-1):\n","                continue\n","            selection = set.intersection(*[texts_by_word[w] for w in potential_arg])  # all texts with all words of potential argument\n","            selection_preds = [predictions[i_] for i_ in selection]\n","            if selection_preds[:-1] == selection_preds[1:]:\n","                    args[selection_preds[0]].add(frozenset(potential_arg))\n","                    minimals[cl][n-1].add(frozenset(potential_arg))\n","    print(potential_args_checked_count, ' potential arg checked.')\n","    return args, minimals"],"metadata":{"id":"CDWZOdPp1e7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_args_naive(args_neg, args_pos):\n","    intersection = set(args_neg) & set(args_pos)\n","    if intersection:\n","        print(\"Check args failed\")\n","        print(len(intersection))\n","        print('intersection = ', intersection)\n","        all_texts = [text for text, _ in dataset]\n","        print(list(intersection)[0][0])\n","        for i, text in enumerate(all_texts):\n","            if list(intersection)[0][0] in text.split():\n","                print(predictions[i])\n","    else:\n","        print(\"Check args successful, intersection is empty\")\n","    print('len(args_neg)=', len(args_neg))\n","    print('len(args_pos)=', len(args_pos))\n","\n","\n","def check_args_consistency(args, dataset, predictions):\n","    all_texts = [text for text, _ in dataset]\n","    for w1, w2 in args:\n","        preds = []\n","        temp_texts = []\n","        for i, text in enumerate(all_texts):\n","            if w1 in text.split() and w2 in text.split():\n","                preds.append(predictions[i])\n","                temp_texts.append(text)\n","                if preds[-1] != preds[0]:\n","                    print(w1, w2, preds)\n","                    pprint(temp_texts)\n","                    assert min(preds) == max(preds)\n","\n","    print('success for consistency')"],"metadata":{"id":"JsuYgcrf1lEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = get_dataset_from_file(\"rt2.train\")\n","predictions = get_predictions(dataset, rt_model)"],"metadata":{"id":"hMn9e7Kw2Hna"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"stopwords\")\n","\n","def clean_texts(texts, rm_stop_words=True, rm_punctuation=True, rm_uniques=False):\n","    split_texts = []\n","    if rm_stop_words:\n","        stop_words = set(stopwords.words('english'))\n","        stop_words.difference_update({'but', 'between', 'again', 'very', 'out', 'most', 'off', 'until', 'more', 'down',\n","                                      'while', 'should', 'both', 'no', 'any', 'then', 'because', 'before', 'then',\n","                                      'because', 'why', 'so', 'not', 'now', 'where ', 'after', 'against', 'further',\n","                                      'than'})\n","        print(stop_words)\n","    else:\n","        stop_words = set()\n","\n","    if rm_punctuation:\n","        stop_words.update({',', '\\'', '.', ';', '--', '(', ')'})\n","    if rm_uniques:\n","        uniques = {}\n","        uniques = defaultdict(lambda: 0, uniques)\n","        for text in texts:\n","            for w in text:\n","                uniques[w] += 1\n","        for k, v in uniques.items():\n","            if v == 1:\n","                stop_words.add(k)\n","\n","    for text in tqdm(texts):\n","        split_texts.append([w for w in text if not w.lower() in stop_words])\n","\n","    return split_texts"],"metadata":{"id":"qO2t4v8Q2R6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary = []\n","all_texts = [text for text, _ in dataset]\n","for text in all_texts:\n","    for word in text.split():\n","        vocabulary.append(word)\n","\n","lens_text = []\n","all_texts = [text for text, _ in dataset]\n","for text in all_texts:\n","    lens_text.append(len(text.split()))\n","print('average len=', np.mean(lens_text))"],"metadata":{"id":"ucNr4RSK2MS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_split_texts = [text.split() for text, _ in dataset]\n","\n","# Preprocessing.\n","print('Preprocessing texts')\n","all_split_texts = clean_texts(all_split_texts, rm_uniques=True)\n","\n","for text in all_split_texts:\n","    lens_text.append(len(text))\n","print('New average len=', np.mean(lens_text))\n","print(len(all_split_texts))\n","\n","print('Initialising texts_by_word...')\n","texts_by_word = dict()\n","for i, text in enumerate(all_split_texts):\n","    for word in set(text):\n","        if word not in texts_by_word:\n","            texts_by_word.update({word: {i}})\n","        else:\n","            texts_by_word[word].add(i)\n","print('texts_by_word initialised')\n","\n","minimals = None\n","anymore_args = True\n","k = 1\n","while anymore_args:\n","    print(\"Generating args length %d (by text):\" % k)\n","    [args_neg, args_pos], minimals = generate_args_lenN_by_text(k, texts_by_word, predictions, minimals)\n","    #pd.to_pickle(minimals[k-1], 'rt2_dev_minimals' + str(k) + '.df')\n","    check_args_naive(args_neg, args_pos)\n","    anymore_args = len(minimals[0][-1]) != 0 or len(minimals[1][-1]) != 0\n","    k += 1"],"metadata":{"id":"Og2VbTzk147n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pprint(minimals)"],"metadata":{"id":"XeuX0Lpc2gpA"},"execution_count":null,"outputs":[]}]}